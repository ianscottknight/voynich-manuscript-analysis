{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fix_notebook_imports\n",
    "\n",
    "from src import util\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import scipy\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WINDOW_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_left_right_error_exception(left_only, right_only): \n",
    "    if left_only and right_only: \n",
    "        raise Exception(\"Only one can be true: left_only or right_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_for_target_word(target_word, context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    check_for_left_right_error_exception(left_only, right_only)\n",
    "    \n",
    "    contexts = []\n",
    "    for i, paragraph in enumerate(util.PARAGRAPHS):\n",
    "        for j, word, in enumerate(paragraph):\n",
    "            if word.lower() == target_word.lower():\n",
    "                start = max(0, j-context_window_size)\n",
    "                end = min(len(paragraph)-1, j+context_window_size+1)\n",
    "                left = paragraph[start:j]\n",
    "                right = paragraph[j+1:end]\n",
    "                if left_only:\n",
    "                    context = left\n",
    "                elif right_only:\n",
    "                    context = right\n",
    "                else:\n",
    "                    context = left + right\n",
    "                if len(context) > 0:\n",
    "                    contexts.append(context)\n",
    "                \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_containing_context_word(context_word, context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    check_for_left_right_error_exception(left_only, right_only)\n",
    "    \n",
    "    contexts = []\n",
    "    for i, paragraph in enumerate(util.PARAGRAPHS):\n",
    "        for j, target_word, in enumerate(paragraph):\n",
    "            start = max(0, j-context_window_size)\n",
    "            end = min(len(paragraph)-1, j+context_window_size+1)\n",
    "            left = paragraph[start:j]\n",
    "            right = paragraph[j+1:end]\n",
    "            if left_only:\n",
    "                context = left\n",
    "            elif right_only:\n",
    "                context = right\n",
    "            else:\n",
    "                context = left + right\n",
    "            if context_word.lower() in [w.lower() for w in context]:\n",
    "                contexts.append(context)\n",
    "                \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readable_contexts_for_target_word(target_word, context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    check_for_left_right_error_exception(left_only, right_only)\n",
    "    \n",
    "    readable_contexts = []\n",
    "    for i, paragraph in enumerate(util.PARAGRAPHS):\n",
    "        for j, word, in enumerate(paragraph):\n",
    "            if word.lower() == target_word.lower():\n",
    "                start = max(0, j-context_window_size)\n",
    "                end = min(len(paragraph)-1, j+context_window_size+1)\n",
    "                left = \" \".join(paragraph[start:j])\n",
    "                right = \" \".join(paragraph[j+1:end])\n",
    "                if left_only:\n",
    "                    readable_context =  f\"{left} <<{target_word}>>\"\n",
    "                elif right_only:\n",
    "                    readable_context = f\"<<{target_word}>> {right}\"\n",
    "                else:\n",
    "                    readable_context = f\"{left} <<{target_word}>> {right}\"\n",
    "                readable_contexts.append(readable_context)\n",
    "                \n",
    "    return readable_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_context_words_counter(target_word_1, target_word_2, context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    counter_1 = get_context_word_counter_for_target_word(target_word_1, context_window_size, left_only=left_only, right_only=right_only)\n",
    "    counter_2 = get_context_word_counter_for_target_word(target_word_2, context_window_size, left_only=left_only, right_only=right_only)\n",
    "    counts = [(key, (round(counter_1[key]/sum(counter_1.values()), 3), round(counter_2[key]/sum(counter_2.values()), 3))) for key in list((counter_1.keys() & counter_2.keys()))]\n",
    "    \n",
    "    counts = sorted(counts, key=lambda x: sum(x[1]), reverse=True)\n",
    "    counter = collections.OrderedDict(counts)\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_counter_for_target_word(target_word, context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    contexts = get_contexts_for_target_word(target_word, context_window_size, left_only=left_only, right_only=right_only)\n",
    "    context_words = [word for context in contexts for word in context]\n",
    "    counter = collections.Counter(context_words)\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_word_to_contexts_dict(context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    target_word_to_contexts_dict = {}\n",
    "    for target_word in util.VOCAB:\n",
    "        target_word_to_contexts_dict[target_word] = get_contexts_for_target_word(target_word, context_window_size, left_only=left_only, right_only=right_only)\n",
    "        \n",
    "    return target_word_to_contexts_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_to_contexts_dict(context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):    \n",
    "    context_word_to_contexts_dict = {}\n",
    "    for context_word in util.VOCAB:\n",
    "        context_word_to_contexts_dict[context_word] = get_contexts_containing_context_word(context_word, context_window_size, left_only=left_only, right_only=right_only)\n",
    "        \n",
    "    return context_word_to_contexts_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_word_probability_dict(context_window_size=CONTEXT_WINDOW_SIZE):\n",
    "    target_word_probability_dict = collections.defaultdict(float)\n",
    "    target_word_counter = collections.Counter(util.WORDS)\n",
    "    num_target_words = len(util.WORDS)\n",
    "    for target_word in util.VOCAB:\n",
    "        target_word_probability_dict[target_word] /= num_target_words\n",
    "        \n",
    "    return target_word_probability_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_probability_dict(context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    context_word_probability_dict = collections.defaultdict(float)\n",
    "    num_contexts = len(util.WORDS)\n",
    "    for target_word in util.VOCAB:\n",
    "        contexts = get_contexts_for_target_word(target_word, context_window_size, left_only=left_only, right_only=right_only)\n",
    "        for context in contexts:\n",
    "            for context_word in list(set(context)):\n",
    "                context_word_probability_dict[context_word] += (1/num_contexts)\n",
    "        \n",
    "    return context_word_probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_probability_given_target_word_dict(context_window_size=CONTEXT_WINDOW_SIZE, left_only=False, right_only=False):\n",
    "    context_word_probability_given_target_word_dict = collections.defaultdict(float)\n",
    "    target_word_counter = collections.Counter(util.WORDS)\n",
    "    for target_word in util.VOCAB:\n",
    "        contexts = get_contexts_for_target_word(target_word, context_window_size, left_only=left_only, right_only=right_only)\n",
    "        for context in contexts:\n",
    "            for context_word in list(set(context)):\n",
    "                context_word_probability_given_target_word_dict[(target_word, context_word)] += (1/target_word_counter[target_word])\n",
    "\n",
    "    return context_word_probability_given_target_word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_to_contexts_dict = get_target_word_to_contexts_dict()\n",
    "target_word_to_left_contexts_dict = get_target_word_to_contexts_dict(left_only=True)\n",
    "target_word_to_right_contexts_dict = get_target_word_to_contexts_dict(right_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_word_to_contexts_dict = get_context_word_to_contexts_dict()\n",
    "context_word_to_left_contexts_dict = get_context_word_to_contexts_dict(left_only=True)\n",
    "context_word_to_right_contexts_dict = get_context_word_to_contexts_dict(right_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_word_probability_dict = get_context_word_probability_dict()\n",
    "left_context_word_probability_dict = get_context_word_probability_dict(left_only=True)\n",
    "right_context_word_probability_dict = get_context_word_probability_dict(right_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_probability_dict = get_target_word_probability_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_word_probability_given_target_word_dict = get_context_word_probability_given_target_word_dict()\n",
    "left_context_word_probability_given_target_word_dict = get_context_word_probability_given_target_word_dict(left_only=True)\n",
    "right_context_word_probability_given_target_word_dict = get_context_word_probability_given_target_word_dict(right_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binomial_probability_of_at_least_k_contexts_containing_context_word(contexts, context_word, verbose=False):\n",
    "    p = context_word_probability_dict[context_word]\n",
    "    n = len(contexts)\n",
    "    k = sum([1 if context_word in context else 0 for context in contexts])\n",
    "    \n",
    "    if k == 0: \n",
    "        prob = 1.0\n",
    "    else:\n",
    "        prob = 1 - scipy.stats.binom.cdf(k-1, n, p)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Binomial probability (assuming independence) of at least as many contexts containing the given context word as was witnessed: {prob}\")\n",
    "        print(\"Binomial parameters:\")\n",
    "        print(f\"\\tp: {round(p, 5)}\")\n",
    "        print(f\"\\tn: {n}\")\n",
    "        print(f\"\\tk: {k}\")\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistically_unlikely_target_word_context_word_pairs(target_word_to_contexts_dict, threshold=1e-5, min_occurrences=10):\n",
    "    results = []\n",
    "    print(f\"Exploring {len(util.VOCAB)} different words...\")\n",
    "    for i, target_word in enumerate(util.VOCAB):\n",
    "        if not sum([1 if w == target_word else 0 for w in util.WORDS]) > min_occurrences: continue\n",
    "        contexts = target_word_to_contexts_dict[target_word]\n",
    "        unique_context_words = list(set([context_word for context in contexts for context_word in context]))\n",
    "        for context_word in unique_context_words:\n",
    "            prob = get_binomial_probability_of_at_least_k_contexts_containing_context_word(contexts, context_word)\n",
    "            if prob < threshold:\n",
    "                results.append((target_word, context_word, prob))\n",
    "    results = sorted(results, key=lambda x: x[2])\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_unlikely_target_word_context_word_pairs = get_statistically_unlikely_target_word_context_word_pairs(target_word_to_contexts_dict)\n",
    "statistically_unlikely_target_word_left_context_word_pairs = get_statistically_unlikely_target_word_context_word_pairs(target_word_to_left_contexts_dict)\n",
    "statistically_unlikely_target_word_right_context_word_pairs = get_statistically_unlikely_target_word_context_word_pairs(target_word_to_right_contexts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_unlikely_target_word_context_word_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(statistically_unlikely_target_word_left_context_word_pairs)\n",
    "print(\"\")\n",
    "pprint(statistically_unlikely_target_word_right_context_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_word_context_word_pair(target_word, context_word):\n",
    "    print(f\"Target word: {target_word}\")\n",
    "    print(f\"Context word: {context_word}\")\n",
    "    print(\"\")\n",
    "    print(f\"Prior probability: {round(context_word_probability_dict[context_word], 5)}\")\n",
    "    print(f\"Posterior probability: {round(context_word_probability_given_target_word_dict[(target_word, context_word)], 5)}\")\n",
    "    print(\"\")\n",
    "    get_binomial_probability_of_at_least_k_contexts_containing_context_word(target_word_to_contexts_dict[target_word], context_word, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"qokeedy\"\n",
    "context_word = \"qokeedy\"\n",
    "\n",
    "analyze_target_word_context_word_pair(target_word, context_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_likely_to_repeat_in_same_context = []\n",
    "\n",
    "MULTIPLIER = 3\n",
    "for word in util.VOCAB: \n",
    "    try:\n",
    "        if context_word_probability_given_target_word_dict[(word, word)] > MULTIPLIER*context_word_probability_dict[word]:\n",
    "            words_likely_to_repeat_in_same_context.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "ratio = len(words_likely_to_repeat_in_same_context) / len(util.VOCAB)\n",
    "print(ratio)    \n",
    "    \n",
    "words_likely_to_repeat_in_same_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = CONTEXT_WINDOW_SIZE\n",
    "min_count = 3\n",
    "epochs = 40\n",
    "\n",
    "model = gensim.models.word2vec.Word2Vec(\n",
    "    util.PARAGRAPHS, \n",
    "    workers=multiprocessing.cpu_count(), \n",
    "    size=vector_size, \n",
    "    window=window, \n",
    "    min_count=min_count,\n",
    "    iter=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_of_pattern(pattern):\n",
    "    return sum([1 for w in util.VOCAB if pattern.lower() in w.lower()]) / len(util.VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binomial_probability_of_at_least_k_similar_word_embeddings_containing_pattern(word, pattern, model, topn=10, verbose=False):    \n",
    "    p = get_probability_of_pattern(pattern)\n",
    "    n = topn\n",
    "    if word in model.wv.vocab:\n",
    "        k = sum([1 for w, v in model.wv.most_similar(word.lower(), topn=n) if pattern.lower() in w.lower()])\n",
    "    else: \n",
    "        k = 0\n",
    "    \n",
    "    if k == 0: \n",
    "        prob = 1.0\n",
    "    else:\n",
    "        prob = 1 - scipy.stats.binom.cdf(k-1, n, p)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Binomial probability (assuming independence) of at least as many similar word embeddings containing the pattern as was witnessed: {prob}\")\n",
    "        print(\"Binomial parameters:\")\n",
    "        print(f\"\\tp: {round(p, 5)}\")\n",
    "        print(f\"\\tn: {n}\")\n",
    "        print(f\"\\tk: {k}\")\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistically_unlikely_patterns_in_similar_word_embeddings(model, threshold=1e-3):\n",
    "    results = []\n",
    "    print(f\"Exploring {len(util.VOCAB)} different words...\")\n",
    "    for i, word in enumerate(util.VOCAB):\n",
    "        already_explored = []\n",
    "        for x,y in itertools.combinations(range(len(word)), r=2):\n",
    "            pattern = word[x:y]\n",
    "            if (word, pattern) in already_explored:\n",
    "                continue\n",
    "            prob = get_binomial_probability_of_at_least_k_similar_word_embeddings_containing_pattern(word, pattern, model)\n",
    "            if prob < threshold:\n",
    "                results.append((word, pattern, prob))\n",
    "            already_explored.append((word, pattern))\n",
    "    results = sorted(results, key=lambda x: x[2])\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_unlikely_patterns_in_similar_word_embeddings = get_statistically_unlikely_patterns_in_similar_word_embeddings(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_unlikely_patterns_in_similar_word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pattern_in_similar_word_embeddings(word, pattern):\n",
    "    print(f\"Reference word: {word}\")\n",
    "    print(f\"Pattern: {pattern}\")\n",
    "    print(\"\")\n",
    "    print(f\"Probability of pattern occurence: {round(get_probability_of_pattern(pattern), 5)}\")\n",
    "    print(\"\")\n",
    "    print(\"Most similar word embeddings:\")\n",
    "    pprint(model.wv.most_similar(word))\n",
    "    print(\"\")\n",
    "    get_binomial_probability_of_at_least_k_similar_word_embeddings_containing_pattern(word, pattern, model, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_pattern_in_similar_word_embeddings(\"tchol\", \"ho\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_pattern_in_similar_word_embeddings(\"lkchedy\", \"ched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
